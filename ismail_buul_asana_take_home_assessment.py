# -*- coding: utf-8 -*-
"""Ismail Buul - Asana Take Home Assessment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gK4LZR91EXxlvczWYuiYk-tkV7qlMUuS

##### Below is a template for Asana's early career data science take-home assessment. Although we encourage candidates to use a similar format as below, feel free to make changes as needed!

### Data Ingestion
"""

import pandas as pd

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pandas as pd

# Read the user data from a CSV file
users = pd.read_csv('https://s3.amazonaws.com/asana-data-interview/takehome_users-intern.csv')

# Read the user engagement data from a CSV file
user_engagement = pd.read_csv('https://s3.amazonaws.com/asana-data-interview/takehome_user_engagement-intern.csv')

"""### 1) Calculating Adoption Rate


"""

# Remove rows with missing or null values
user_engagement = user_engagement.dropna()

# Convert timestamp columns to datetime objects
user_engagement['time_stamp'] = pd.to_datetime(user_engagement['time_stamp'])

# Group the data by user and calculate the number of days each user has logged in
user_engagement = user_engagement.groupby('user_id')['time_stamp'].nunique()

# Filter the data to only include users who have logged in on at least three separate days in a seven-day period
user_engagement = user_engagement.astype(int)
adopted_users = user_engagement[user_engagement >= 3]

# Calculate the adoption rate
adoption_rate = len(adopted_users) / len(user_engagement)

"""### 2) Methodology"""

import os

# Check if the takehome_users.csv file exists at the specified path
if os.path.exists('/path/to/takehome_users.csv'):
    # If the file exists, read the user data from the takehome_users.csv file
    user_data = pd.read_csv('/path/to/takehome_users.csv', encoding='latin-1')

import glob
import pandas as pd

# Replace this with the correct path to the directory containing your data files
data_dir = 'C:/Users/<user>/Downloads/'

# Use glob.glob() to list only files with the .csv file extension
data_files = glob.glob(data_dir + '*.csv')

# Loop over the files in the data directory
for data_file in data_files:
    # Read the data from the file
    data = pd.read_csv(data_file)
    
    # Map the values in the 'creation_source' column
    data['creation_source'] = data['creation_source'].map({
        'PERSONAL_PROJECTS': 0,
        'GUEST_INVITE': 1,
        'ORG_INVITE': 2,
        'SIGNUP': 3,
        'SIGNUP_GOOGLE_AUTH': 4
    })

    # Print the first few rows of data
    print(data.head())

"""##### 2a) Writeup associated with methodology

2) Methodology

To determine which factors predict user adoption, we will need to use a machine learning model. We can train a model on the user data, using the adopted/not adopted status of each user as the target variable.

First, we will need to prepare the data for modeling. This may involve:

Encoding any categorical variables (such as creation_source) as numerical data
Splitting the data into training and testing sets
Standardizing the data to have a mean of 0 and a standard deviation of 1
Once the data is prepared, we can train a model such as a logistic regression or a decision tree on the training set. Then, we can use the trained model to make predictions on the testing set and evaluate its performance.

### 3) What Factors Predict User Adoption?
"""

import glob
import pandas as pd

# Replace this with the correct path to the directory containing your data files
data_dir = 'C:/Users/<user>/Downloads/'

# Use glob.glob() to list only files with the .csv file extension
data_files = glob.glob(data_dir + '*.csv')

# Loop over the files in the data directory
for data_file in data_files:
    # Read the data from the file
    data = pd.read_csv(data_file)

    # Merge the user data and user engagement data on the user_id column
    data = user_data.merge(user_engagement, on='user_id')

import glob
import pandas as pd

# Replace this with the correct path to the directory containing your data files
data_dir = 'C:/Users/<user>/Downloads/'

# Use glob.glob() to list only files with the .csv file extension
data_files = glob.glob(data_dir + '*.csv')

# Loop over the files in the data directory
for data_file in data_files:
    # Read the data from the file
    data = pd.read_csv(data_file)

    # Encode the creation_source column as numeric data
    data['creation_source'] = data['creation_source'].map({'PERSONAL_PROJECTS': 1, 'GUEST_INVITE': 2, 'ORG_INVITE': 3, 'SIGNUP': 4, 'SIGNUP_GOOGLE_AUTH': 5})

from sklearn.model_selection import train_test_split
import glob
import pandas as pd

# Replace this with the correct path to the directory containing your data files
data_dir = 'C:/Users/<user>/Downloads/'

# Use glob.glob() to list only files with the .csv file extension
data_files = glob.glob(data_dir + '*.csv')

# Loop over the files in the data directory
for data_file in data_files:
    # Read the data from the file
    data = pd.read_csv(data_file)

    # Split the data into training and testing sets
    train, test = train_test_split(data, test_size=0.2)

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import glob
import pandas as pd

# Replace this with the correct path to the directory containing your data files
data_dir = 'C:/Users/<user>/Downloads/'

# Use glob.glob() to list only files with the .csv file extension
data_files = glob.glob(data_dir + '*.csv')

# Loop over the files in the data directory
for data_file in data_files:
    # Read the data from the file
    data = pd.read_csv(data_file)

    # Split the data into training and testing sets
    train, test = train_test_split(data, test_size=0.2)

    # Standardize the data
    scaler = StandardScaler()
    train_scaled = scaler.fit_transform(train)
    test_scaled = scaler.transform(test)

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import glob
import pandas as pd

# Replace this with the correct path to the directory containing your data files
data_dir = 'C:/Users/<user>/Downloads/'

# Use glob.glob() to list only files with the .csv file extension
data_files = glob.glob(data_dir + '*.csv')

# Loop over the files in the data directory
for data_file in data_files:
    # Read the data from the file
    data = pd.read_csv(data_file)

    # Split the data into training and testing sets
    train, test = train_test_split(data, test_size=0.2)

    # Standardize the data
    scaler = StandardScaler()
    train_scaled = scaler.fit_transform(train)
    test_scaled = scaler.transform(test)

    # Train a logistic regression model on the training set
    model = LogisticRegression()
    model.fit(train_scaled, train['adopted'])

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import glob
import pandas as pd

# Replace this with the correct path to the directory containing your data files
data_dir = 'C:/Users/<user>/Downloads/'

# Use glob.glob() to list only files with the .csv file extension
data_files = glob.glob(data_dir + '*.csv')

# Loop over the files in the data directory
for data_file in data_files:
    # Read the data from the file
    data = pd.read_csv(data_file)

    # Split the data into training and testing sets
    train, test = train_test_split(data, test_size=0.2)

    # Standardize the data
    scaler = StandardScaler()
    train_scaled = scaler.fit_transform(train)
    test_scaled = scaler.transform(test)

    # Train a logistic regression model on the training set
    model = LogisticRegression()
    model.fit(train_scaled, train['adopted'])

    # Make predictions on the testing set
    predictions = model.predict(test_scaled)

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import glob
import pandas as pd

# Replace this with the correct path to the directory containing your data files
data_dir = 'C:/Users/<user>/Downloads/'

# Use glob.glob() to list only files with the .csv file extension
data_files = glob.glob(data_dir + '*.csv')

# Loop over the files in the data directory
for data_file in data_files:
    # Read the data from the file
    data = pd.read_csv(data_file)

    # Split the data into training and testing sets
    train, test = train_test_split(data, test_size=0.2)

    # Standardize the data
    scaler = StandardScaler()
    train_scaled = scaler.fit_transform(train)
    test_scaled = scaler.transform(test)

    # Train a logistic regression model on the training set
    model = LogisticRegression()
    model.fit(train_scaled, train['adopted'])

    # Make predictions on the testing set
    predictions = model.predict(test_scaled)

    # Evaluate the model's performance
    print(classification_report(test['adopted'], predictions))

"""##### 3a) Writeup associated with what factors predict user adoption?

Using machine learning, we can identify which factors are important for predicting user adoption. Some potential predictors of user adoption include:

The method by which the user signed up for the product (e.g. personal invite, guest invite, etc.)
The user's email domain (e.g. Gmail, Yahoo, etc.)
The user's organization (if applicable)
Whether the user has opted into receiving marketing emails
Whether the user is on the regular marketing email drip
The user's last session creation time
By training a machine learning model on user data, we can determine the relative importance of these and other factors in predicting user adoption. This information can be used to improve the long-term success of users by identifying which factors are most important and focusing on those in future initiatives.

### 4) Additional Commentary (Optional)

One potential limitation of this analysis is that it only considers users who have logged in to the product at least once. This means that users who signed up for the product but never actually used it are not included in the analysis. This could potentially bias the results, as these users may have different characteristics than those who did use the product.

Additionally, the definition of an "adopted" user used in this analysis (a user who has logged in on three separate days in at least one seven-day period) may not accurately reflect the long-term success of the user. It is possible that a user could meet this criterion but still not be successful at using the product in the long term. A more accurate measure of long-term success may involve looking at factors such as the number of tasks completed or the amount of time spent using the product on a regular basis.

In conclusion, using machine learning to predict user adoption can provide valuable insights into which factors are important for long-term success. However, it is important to carefully consider the limitations of the data and the definition of an adopted user in order to ensure that the results are reliable and accurate.



Side note: I started using the glob module to list the files in the directory that match a specific pattern because I had a tough time with the data.csv and I iterate over each file in the list and read the data from each file.
"""