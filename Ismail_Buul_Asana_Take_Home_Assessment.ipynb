{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs5ozOi3LTJP"
      },
      "source": [
        "##### Below is a template for Asana's early career data science take-home assessment. Although we encourage candidates to use a similar format as below, feel free to make changes as needed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OEuDhvFLTJR"
      },
      "source": [
        "### Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "JuZcDC2ELTJR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "8rlyqeNCLTJS"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Read the user data from a CSV file\n",
        "users = pd.read_csv('https://s3.amazonaws.com/asana-data-interview/takehome_users-intern.csv')\n",
        "\n",
        "# Read the user engagement data from a CSV file\n",
        "user_engagement = pd.read_csv('https://s3.amazonaws.com/asana-data-interview/takehome_user_engagement-intern.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0b2XmnGLTJS"
      },
      "source": [
        "### 1) Calculating Adoption Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YK0IvA3wLTJS"
      },
      "outputs": [],
      "source": [
        "# Remove rows with missing or null values\n",
        "user_engagement = user_engagement.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XsJUB35xLTJS"
      },
      "outputs": [],
      "source": [
        "# Convert timestamp columns to datetime objects\n",
        "user_engagement['time_stamp'] = pd.to_datetime(user_engagement['time_stamp'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "T9am4syHLTJT"
      },
      "outputs": [],
      "source": [
        "# Group the data by user and calculate the number of days each user has logged in\n",
        "user_engagement = user_engagement.groupby('user_id')['time_stamp'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HIKYG2jzLTJT"
      },
      "outputs": [],
      "source": [
        "# Filter the data to only include users who have logged in on at least three separate days in a seven-day period\n",
        "user_engagement = user_engagement.astype(int)\n",
        "adopted_users = user_engagement[user_engagement >= 3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the adoption rate\n",
        "adoption_rate = len(adopted_users) / len(user_engagement)"
      ],
      "metadata": {
        "id": "GJlS9wyhMn74"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPKQcE6oLTJT"
      },
      "source": [
        "### 2) Methodology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3GWJB4cdLTJT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check if the takehome_users.csv file exists at the specified path\n",
        "if os.path.exists('/path/to/takehome_users.csv'):\n",
        "    # If the file exists, read the user data from the takehome_users.csv file\n",
        "    user_data = pd.read_csv('/path/to/takehome_users.csv', encoding='latin-1')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "PG8eCu5FLTJT"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this with the correct path to the directory containing your data files\n",
        "data_dir = 'C:/Users/<user>/Downloads/'\n",
        "\n",
        "# Use glob.glob() to list only files with the .csv file extension\n",
        "data_files = glob.glob(data_dir + '*.csv')\n",
        "\n",
        "# Loop over the files in the data directory\n",
        "for data_file in data_files:\n",
        "    # Read the data from the file\n",
        "    data = pd.read_csv(data_file)\n",
        "    \n",
        "    # Map the values in the 'creation_source' column\n",
        "    data['creation_source'] = data['creation_source'].map({\n",
        "        'PERSONAL_PROJECTS': 0,\n",
        "        'GUEST_INVITE': 1,\n",
        "        'ORG_INVITE': 2,\n",
        "        'SIGNUP': 3,\n",
        "        'SIGNUP_GOOGLE_AUTH': 4\n",
        "    })\n",
        "\n",
        "    # Print the first few rows of data\n",
        "    print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6dks4miLTJU"
      },
      "source": [
        "##### 2a) Writeup associated with methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzjvuwByLTJU"
      },
      "source": [
        "2) Methodology\n",
        "\n",
        "To determine which factors predict user adoption, we will need to use a machine learning model. We can train a model on the user data, using the adopted/not adopted status of each user as the target variable.\n",
        "\n",
        "First, we will need to prepare the data for modeling. This may involve:\n",
        "\n",
        "Encoding any categorical variables (such as creation_source) as numerical data\n",
        "Splitting the data into training and testing sets\n",
        "Standardizing the data to have a mean of 0 and a standard deviation of 1\n",
        "Once the data is prepared, we can train a model such as a logistic regression or a decision tree on the training set. Then, we can use the trained model to make predictions on the testing set and evaluate its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2DUp-bcLTJU"
      },
      "source": [
        "### 3) What Factors Predict User Adoption?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "NPc3u5_CLTJU"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this with the correct path to the directory containing your data files\n",
        "data_dir = 'C:/Users/<user>/Downloads/'\n",
        "\n",
        "# Use glob.glob() to list only files with the .csv file extension\n",
        "data_files = glob.glob(data_dir + '*.csv')\n",
        "\n",
        "# Loop over the files in the data directory\n",
        "for data_file in data_files:\n",
        "    # Read the data from the file\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Merge the user data and user engagement data on the user_id column\n",
        "    data = user_data.merge(user_engagement, on='user_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "aHcpIDSGLTJU"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this with the correct path to the directory containing your data files\n",
        "data_dir = 'C:/Users/<user>/Downloads/'\n",
        "\n",
        "# Use glob.glob() to list only files with the .csv file extension\n",
        "data_files = glob.glob(data_dir + '*.csv')\n",
        "\n",
        "# Loop over the files in the data directory\n",
        "for data_file in data_files:\n",
        "    # Read the data from the file\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Encode the creation_source column as numeric data\n",
        "    data['creation_source'] = data['creation_source'].map({'PERSONAL_PROJECTS': 1, 'GUEST_INVITE': 2, 'ORG_INVITE': 3, 'SIGNUP': 4, 'SIGNUP_GOOGLE_AUTH': 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "rA8Ri7jTLTJU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this with the correct path to the directory containing your data files\n",
        "data_dir = 'C:/Users/<user>/Downloads/'\n",
        "\n",
        "# Use glob.glob() to list only files with the .csv file extension\n",
        "data_files = glob.glob(data_dir + '*.csv')\n",
        "\n",
        "# Loop over the files in the data directory\n",
        "for data_file in data_files:\n",
        "    # Read the data from the file\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train, test = train_test_split(data, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "SQZojljBLTJU"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this with the correct path to the directory containing your data files\n",
        "data_dir = 'C:/Users/<user>/Downloads/'\n",
        "\n",
        "# Use glob.glob() to list only files with the .csv file extension\n",
        "data_files = glob.glob(data_dir + '*.csv')\n",
        "\n",
        "# Loop over the files in the data directory\n",
        "for data_file in data_files:\n",
        "    # Read the data from the file\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    train_scaled = scaler.fit_transform(train)\n",
        "    test_scaled = scaler.transform(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "aDsxwtr0LTJU"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this with the correct path to the directory containing your data files\n",
        "data_dir = 'C:/Users/<user>/Downloads/'\n",
        "\n",
        "# Use glob.glob() to list only files with the .csv file extension\n",
        "data_files = glob.glob(data_dir + '*.csv')\n",
        "\n",
        "# Loop over the files in the data directory\n",
        "for data_file in data_files:\n",
        "    # Read the data from the file\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    train_scaled = scaler.fit_transform(train)\n",
        "    test_scaled = scaler.transform(test)\n",
        "\n",
        "    # Train a logistic regression model on the training set\n",
        "    model = LogisticRegression()\n",
        "    model.fit(train_scaled, train['adopted'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this with the correct path to the directory containing your data files\n",
        "data_dir = 'C:/Users/<user>/Downloads/'\n",
        "\n",
        "# Use glob.glob() to list only files with the .csv file extension\n",
        "data_files = glob.glob(data_dir + '*.csv')\n",
        "\n",
        "# Loop over the files in the data directory\n",
        "for data_file in data_files:\n",
        "    # Read the data from the file\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    train_scaled = scaler.fit_transform(train)\n",
        "    test_scaled = scaler.transform(test)\n",
        "\n",
        "    # Train a logistic regression model on the training set\n",
        "    model = LogisticRegression()\n",
        "    model.fit(train_scaled, train['adopted'])\n",
        "\n",
        "    # Make predictions on the testing set\n",
        "    predictions = model.predict(test_scaled)\n"
      ],
      "metadata": {
        "id": "JMxWF6i1RUm8"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this with the correct path to the directory containing your data files\n",
        "data_dir = 'C:/Users/<user>/Downloads/'\n",
        "\n",
        "# Use glob.glob() to list only files with the .csv file extension\n",
        "data_files = glob.glob(data_dir + '*.csv')\n",
        "\n",
        "# Loop over the files in the data directory\n",
        "for data_file in data_files:\n",
        "    # Read the data from the file\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    train_scaled = scaler.fit_transform(train)\n",
        "    test_scaled = scaler.transform(test)\n",
        "\n",
        "    # Train a logistic regression model on the training set\n",
        "    model = LogisticRegression()\n",
        "    model.fit(train_scaled, train['adopted'])\n",
        "\n",
        "    # Make predictions on the testing set\n",
        "    predictions = model.predict(test_scaled)\n",
        "\n",
        "    # Evaluate the model's performance\n",
        "    print(classification_report(test['adopted'], predictions))\n"
      ],
      "metadata": {
        "id": "snZKu6ctRhCP"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STakYOdQLTJU"
      },
      "source": [
        "##### 3a) Writeup associated with what factors predict user adoption?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgD_CGQPLTJU"
      },
      "source": [
        "Using machine learning, we can identify which factors are important for predicting user adoption. Some potential predictors of user adoption include:\n",
        "\n",
        "The method by which the user signed up for the product (e.g. personal invite, guest invite, etc.)\n",
        "The user's email domain (e.g. Gmail, Yahoo, etc.)\n",
        "The user's organization (if applicable)\n",
        "Whether the user has opted into receiving marketing emails\n",
        "Whether the user is on the regular marketing email drip\n",
        "The user's last session creation time\n",
        "By training a machine learning model on user data, we can determine the relative importance of these and other factors in predicting user adoption. This information can be used to improve the long-term success of users by identifying which factors are most important and focusing on those in future initiatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oiOTTHMLTJV"
      },
      "source": [
        "### 4) Additional Commentary (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg1XMixhLTJV"
      },
      "source": [
        "One potential limitation of this analysis is that it only considers users who have logged in to the product at least once. This means that users who signed up for the product but never actually used it are not included in the analysis. This could potentially bias the results, as these users may have different characteristics than those who did use the product.\n",
        "\n",
        "Additionally, the definition of an \"adopted\" user used in this analysis (a user who has logged in on three separate days in at least one seven-day period) may not accurately reflect the long-term success of the user. It is possible that a user could meet this criterion but still not be successful at using the product in the long term. A more accurate measure of long-term success may involve looking at factors such as the number of tasks completed or the amount of time spent using the product on a regular basis.\n",
        "\n",
        "In conclusion, using machine learning to predict user adoption can provide valuable insights into which factors are important for long-term success. However, it is important to carefully consider the limitations of the data and the definition of an adopted user in order to ensure that the results are reliable and accurate.\n",
        "\n",
        "\n",
        "\n",
        "Side note: I started using the glob module to list the files in the directory that match a specific pattern because I had a tough time with the data.csv and I iterate over each file in the list and read the data from each file. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}